# app/transcription.py
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()  # Make sure OPENAI_API_KEY is loaded

# Set the API key for OpenAI
openai_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_key)

def transcription(audio_path):
    """
    Transcribe an audio file using OpenAI's Whisper model.

    :param audio_path: Path to the audio file.
    :return: Transcribed text from the audio.
    """
    with open(audio_path, "rb") as audio_file:
        # Create a transcription using the specified model.
        transcription_response = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file
        )
    return transcription_response.text

def action_items(transcript):
    """
    Generate a summary and a list of action items for a doctor's visit transcript.

    :param transcript: The transcript text of the consultation.
    :return: Action items generated by the chat model.
    """
    prompt = (
        "You are going to get an audio transcript of a doctor's visit for diabetes. "
        "In the audio transcript, there is audio of both the doctor and patient. "
        "You are basically a medically educated scribe assistant for the doctor. "
        "The patient wants a summary and a list of action items based on the doctor's visits."
    )
    completion = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "developer", "content": prompt},
            {"role": "user", "content": "Recommend action items for this visit: " + str(transcript)}
        ]
    )
    return completion.choices[0].message.content
